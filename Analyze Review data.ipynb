{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvIyW_hyaOjD"
      },
      "source": [
        "## import library requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5scNNadpoIf",
        "outputId": "648c94c2-09db-487d-858f-2cec1f19496f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Pickle text data for generating text\n",
        "import pickle\n",
        "\n",
        "# polarity and subjectivity sentiment\n",
        "from textblob import TextBlob\n",
        "\n",
        "# create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# markov chaain model for generating\n",
        "from gensim import matutils, models\n",
        "import scipy.sparse\n",
        "\n",
        "# Create a new document-term matrix using only nouns\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "import random\n",
        "\n",
        "# text cleaning techniques\n",
        "import re\n",
        "import string\n",
        "\n",
        "# stopword cleaning and tokenizing\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# pull out nouns from a string of text\n",
        "from nltk import pos_tag\n",
        "\n",
        "# corpus downnload\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# column width Visual adjustment\n",
        "pd.set_option('max_colwidth',150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh9lT36wjsEf"
      },
      "source": [
        "# Preview Initial Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "z6jmShfpiKze",
        "outputId": "8be66317-7144-4b1c-a3c4-14eedb632ebc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3a92bf9010d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gamestop_product_reviews_dataset_sample.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gamestop_product_reviews_dataset_sample.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/gamestop_product_reviews_dataset_sample.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEjz3CR-jhvO"
      },
      "source": [
        "# preview data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOZvyEoPcl3O"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"number of reviews\", df.shape[0])\n",
        "print(\"number of columns\", df.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0E8qoq4aTk2"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Zy-WB-hyye"
      },
      "outputs": [],
      "source": [
        "# clear a bunch of unwanted columns\n",
        "review_data = df[['name', 'brand', 'review_title',\n",
        "    'review_description', 'recommended_review', 'rating',\n",
        "    'average_rating', 'reviews_count']]\n",
        "# preview chopped new features\n",
        "print(\"selected review features for data acquisition\", list(review_data.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaCH6CbrUAVo"
      },
      "source": [
        "format observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJt30pPDbCyz"
      },
      "outputs": [],
      "source": [
        "# check brand column\n",
        "print(\"number of brands\", len(review_data.brand.unique()))\n",
        "review_data.brand.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm5iJrMwYgPS"
      },
      "outputs": [],
      "source": [
        "# check column recomended review format \n",
        "review_data.recommended_review.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOsQMQFfY6_x"
      },
      "outputs": [],
      "source": [
        "# check rating column format\n",
        "review_data.rating.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YRaUYBBZcJd"
      },
      "outputs": [],
      "source": [
        "review_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1P5jrD3DiGC"
      },
      "source": [
        "# Data acquisition\n",
        "\n",
        "* Binarize reccomended review column form and positive recomended sentiment column \n",
        "\n",
        "* Aggregate the sum of reccomended boolean values and calculate a positive reccomended sentiment\n",
        "\n",
        "* Aggregate remaining columns by name remaining columns (average rating, review count and brand)\n",
        "\n",
        "* Aggregate text columns by name from the main data source\n",
        "\n",
        "* Concat datasets and save acquired data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtaYioGkVAO"
      },
      "source": [
        "> Binarize reccomended review column "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6f19POclLps"
      },
      "outputs": [],
      "source": [
        "\n",
        "    \n",
        "# sort the data by name \n",
        "sorting_data = review_data.sort_values('name', ascending = True).set_index('name')\n",
        "    \n",
        "# Split the string result is yes and no\n",
        "temp = sorting_data.recommended_review.str.split(':', expand=True)[0].to_frame()\n",
        "# name the new dataframe column\n",
        "temp.columns = ['recommended_review']\n",
        "# preview\n",
        "temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze5DUrYKjNVI"
      },
      "outputs": [],
      "source": [
        "# Encode  the datasets to 1 and 0\n",
        "encoded = temp.applymap(hot_encode)\n",
        "# Create two boolean columns of yes and no reccomended \n",
        "one_hot = pd.get_dummies(temp,drop_first=False)\n",
        "# preview\n",
        "one_hot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zPtInWWpQRk"
      },
      "outputs": [],
      "source": [
        "# drop original data column and replace with new dummies\n",
        "acquire_recomended_review = sorting_data.drop(columns = 'recommended_review').copy()\n",
        "# concat dummies\n",
        "acquire_recomended_review = pd.concat([acquire_recomended_review, one_hot], axis =1)\n",
        "# preview cleaning data\n",
        "acquire_recomended_review.head(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWLQhc2zEJVv"
      },
      "source": [
        "> Aggregate the sum of reccomended boolean values and calculate a positive reccomended sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igQlnbQeD1tt"
      },
      "outputs": [],
      "source": [
        "# Group Items and sum number of recomended then concat\n",
        "grouped_not_recommended = acquire_recomended_review.groupby('name')['recommended_review_No'].sum().to_frame().sort_values('name', ascending = True)\n",
        "grouped_recommended = acquire_recomended_review.groupby('name')['recommended_review_yes'].sum().to_frame().sort_values('name', ascending = True)\n",
        "# create a new dataframe\n",
        "summed_recommended = pd.concat([grouped_not_recommended,grouped_recommended.recommended_review_yes], axis=1).sort_values('name', ascending = True)\n",
        "\n",
        "# preview new data\n",
        "summed_recommended.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-o9Ag-BmBuA"
      },
      "outputs": [],
      "source": [
        "# make column of positive percent of recomendations\n",
        "pList = []\n",
        "# iterating over rows using iterrows() calculate percent sentiment\n",
        "for i, j in summed_recommended.iterrows():\n",
        "  # yes/(no + sum)\n",
        "  pList.append(j[1]/(j[0]+j[1]))\n",
        "\n",
        "# make neew column and fill with percent sentiment list\n",
        "summed_recommended['recommended_sentiment'] = pList\n",
        "# give a new index\n",
        "# Recommended = summed_recommended.sort_values('recommended_review_yes', ascending = False).reset_index()\n",
        "summed_recommended.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s8zPb0Vt0EC"
      },
      "source": [
        "Aggregate text columns by name from the main data source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HxyqPyNkXFM"
      },
      "outputs": [],
      "source": [
        "# Group up the review description by the name and sum all the sentences to one column\n",
        "aggregated_reviews = df.groupby('name')['review_description'].sum().to_frame().apply(' '.join, axis=1)\n",
        "\n",
        "# Dataframe\n",
        "aggregated_reviews = aggregated_reviews.to_frame()\n",
        "aggregated_reviews.rename(columns = {0:'review'}, inplace = True)\n",
        "# preview frame\n",
        "aggregated_reviews.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNyr_QwLCKkP"
      },
      "source": [
        "Aggregate remaining columns by name average rating, review count and brand "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1jga5fAw0VE"
      },
      "outputs": [],
      "source": [
        "#list top product names\n",
        "pList = review_data.sort_values('name', ascending = True).name.unique()\n",
        "\n",
        "# average rating list\n",
        "rList = []\n",
        "# count list\n",
        "cList = []\n",
        "# brand list\n",
        "bList = []\n",
        "\n",
        "# iterate through main and retrieve average rating, review count and brand list\n",
        "for i in pList:\n",
        "  # average rating list\n",
        "  rList.append(review_data.loc[review_data['name'] == i].average_rating.unique())\n",
        "  # review count list\n",
        "  cList.append(review_data.loc[review_data['name'] == i].reviews_count.unique())\n",
        "  # brand list\n",
        "  bList.append(review_data.loc[review_data['name'] == i].brand.unique())\n",
        "\n",
        "# format dict for dataframe\n",
        "data = {\n",
        "    'name': pList,\n",
        "    'brand' : bList,\n",
        "    'average_rating': rList,\n",
        "    'reviews_count' : cList\n",
        "}\n",
        "\n",
        "# make new dataframe with collected lists sort by name alphabetical order\n",
        "new_data = pd.DataFrame(data).sort_values('name', ascending = True)\n",
        "# make new columns and fill with data \n",
        "new_data['average_rating'] = new_data['average_rating'].astype(float)\n",
        "new_data['reviews_count'] = new_data['reviews_count'].astype(int) \n",
        "new_data['brand'] = new_data['brand'].astype(str).str.replace(\"[\", \"\",regex=True).str.replace(\"]\", \"\",regex=True).str.replace(\"'\", \"\",regex=True)\n",
        "# preview new data\n",
        "new_data = new_data.set_index('name')\n",
        "new_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeX7HJVXWJ0t"
      },
      "source": [
        "> Concat datasets and save acquired data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvqyNhr7V_bB"
      },
      "outputs": [],
      "source": [
        "# concat new_data to aggregated_reviews\n",
        "acquired_data = pd.concat([aggregated_reviews, new_data], join = \"inner\", axis =1)\n",
        "# concat recomended dataset\n",
        "completed_acquisition = pd.concat([acquired_data, summed_recommended], join = \"inner\", axis =1)\n",
        "\n",
        "# save acquired dataset\n",
        "completed_acquisition.to_csv('/content/drive/MyDrive/Data files/GamestopFile/completed_acquisition.csv')\n",
        "\n",
        "# preview data\n",
        "completed_acquisition.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7QrgAzSbmyH"
      },
      "source": [
        "> save raw text corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQdpdJEgbl0_"
      },
      "outputs": [],
      "source": [
        "# save raw text for future generating reviews\n",
        "raw_text = completed_acquisition.reset_index()\n",
        "raw_text\n",
        "\n",
        "raw_text = raw_text[['name','review']].set_index('name')\n",
        "raw_text.to_pickle('/content/drive/MyDrive/Data files/GamestopFile/rawText_corpus.pkl')\n",
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjjfIpcXDLG2"
      },
      "source": [
        "# Visually analyse Recomended sentiment relative to the number of reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKFHefV1jpnW"
      },
      "outputs": [],
      "source": [
        "# consider reviews with more then 100 review count\n",
        "sufficient_reviews = completed_acquisition[completed_acquisition.reviews_count >= 100]\n",
        "print(\"number of products with more then 100 reviews\", len(sufficient_reviews))\n",
        "# Sort new list by number of reviews\n",
        "most_reviews = sufficient_reviews.sort_values(\"reviews_count\", ascending = False)\n",
        "pos_recomendation = sufficient_reviews.sort_values(\"recommended_sentiment\", ascending = False)\n",
        "sufficient_reviews.to_csv(\"/content/drive/MyDrive/Data files/GamestopFile/sufficient_reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCDtFVWjGfSx"
      },
      "outputs": [],
      "source": [
        "print(\"top 5 positively recomended\")\n",
        "print(pos_recomendation[\"recommended_sentiment\"].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUjMFJZxGbK9"
      },
      "outputs": [],
      "source": [
        "print(\"top 5 most review count\")\n",
        "print(most_reviews[\"reviews_count\"].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLO7SLUaUHoc"
      },
      "outputs": [],
      "source": [
        "# plot a bar graph according to the product sentiment\n",
        "rBar = pos_recomendation[[\"recommended_sentiment\"]]\n",
        "sns.barplot(x = rBar.index, y = rBar.recommended_sentiment)\n",
        "\n",
        "labels = rBar.index.tolist()\n",
        "plt.gcf().set_size_inches(15, 7)\n",
        "\n",
        "# plot asthetics\n",
        "plt.title('Positive recommendation sentiment', fontsize = 20)\n",
        "plt.xlabel('Most popular products', fontsize = 15)\n",
        "plt.ylabel('Sentiment', fontsize = 15)\n",
        "\n",
        "plt.xticks(ticks = range(len(rBar)) ,labels = labels, rotation = '90')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cc-mgQukXBd"
      },
      "source": [
        "# Text Preperation\n",
        "\n",
        "* Create cleaning funtions: Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.\n",
        "\n",
        "* Apply text cleaning methods\n",
        "\n",
        "* Select text data and pickle texgt corpus\n",
        "\n",
        "* Remove stop words and pickle stopwords\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUCLU_OWjR3o"
      },
      "outputs": [],
      "source": [
        "# # text cleaning techniques\n",
        "# import re\n",
        "# import string\n",
        "\n",
        "# text cleaning 1\n",
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "# text cleaning 2\n",
        "def clean_text_round2(text):\n",
        "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "\n",
        "round1 = lambda x: clean_text_round1(x)\n",
        "round2 = lambda x: clean_text_round2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F682a9YekIc"
      },
      "source": [
        "Apply text cleaning methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiBFn5TQjRrD"
      },
      "outputs": [],
      "source": [
        "# apply cleaning step 1\n",
        "# Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.\n",
        "clean_text = pd.DataFrame(most_reviews.review.apply(round1))\n",
        "# apply cleaning step 2\n",
        "# Get rid of some additional punctuation and non-sensical text that was missed the first time around.\n",
        "clean_text = pd.DataFrame(clean_text.review.apply(round2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmftYFluoIRk"
      },
      "source": [
        "select text data and save a corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnkNe5dMAkLa"
      },
      "outputs": [],
      "source": [
        "# copy data for clean text column\n",
        "text_cleaning = most_reviews.copy()\n",
        "text_cleaning = text_cleaning.drop(columns = 'review')\n",
        "text_cleaning['review'] = clean_text.review\n",
        "\n",
        "\n",
        "# shift column to first position\n",
        "shiftCol= text_cleaning.pop('review')\n",
        "# insert column to the front\n",
        "text_cleaning.insert(0, 'review', shiftCol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCLb-EjYrWIN"
      },
      "outputs": [],
      "source": [
        "# Save cleaned text corpus\n",
        "text_cleaning['review'].to_pickle('/content/drive/MyDrive/Data files/GamestopFile/clean_text_corpus.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Jf8sMpJbF"
      },
      "source": [
        "Remove stop words and save stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qMcJ7slHszE"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "text=text_cleaning['review'].to_frame()\n",
        "stop_step = text_cleaning.index\n",
        "stop = set(stopwords.words('english'))\n",
        "# record stopwords\n",
        "pickle.dump(stop, open(\"/content/drive/MyDrive/Data files/GamestopFile/stopwords.pkl\", \"wb\"))\n",
        "\n",
        "text['review_without_stopwords'] = text_cleaning['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxSOj-85J0nl"
      },
      "outputs": [],
      "source": [
        "textClean = text.drop(columns = ('review')).rename(columns = {'review_without_stopwords':'review'})\n",
        "textClean.to_pickle(\"/content/drive/MyDrive/Data files/GamestopFile/clean_text_no_stop_corpus.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3r6gCDQBZVC"
      },
      "source": [
        "# Product Review Sentiment\n",
        "> vectorizer \n",
        "\n",
        "> format vectorized words to a Document term matrix according to the product names and word occurences.\n",
        "\n",
        "> Transpose list of words to index.\n",
        "\n",
        "> Stem transposed list and replace index with new stemmed words\n",
        "\n",
        "> Filter More words according to repitition for word cloud visualisation.\n",
        "\n",
        "> Save clean data as csv\n",
        "\n",
        "> Evaluate sentiment analysis on product titles.\n",
        "\n",
        "> Plot word cloud for visual word significant representation in product reviews  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "050dQbNHLjfK"
      },
      "source": [
        "Document term matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUYCM4gsph8V"
      },
      "outputs": [],
      "source": [
        "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# drop stop words \n",
        "cv = CountVectorizer(stop_words='english')\n",
        "dataCV = cv.fit_transform(textClean.review)\n",
        "\n",
        "# Set new dataframe\n",
        "dataDTM = pd.DataFrame(dataCV.toarray(), columns=cv.get_feature_names())\n",
        "dataDTM.index = textClean.index\n",
        "\n",
        "# record stopwords\n",
        "pickle.dump(cv, open(\"/content/drive/MyDrive/Data files/GamestopFile/stopwords.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nEee96apjXK"
      },
      "outputs": [],
      "source": [
        "# transpose text \n",
        "data = dataDTM.transpose()\n",
        "print(\" number of words\", data.shape[0])\n",
        "# preview transposed dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlHlOmA1SwyW"
      },
      "source": [
        "# Preview and choose a Stemming and lemmatizing method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS2fba3SGZQ5"
      },
      "source": [
        "Textblob stemming and lemmatizing with pos tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuOHKiBh5S5P"
      },
      "outputs": [],
      "source": [
        "# # Lemmatize with POS Tag\n",
        "# from nltk.corpus import wordnet\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# from nltk.stem import \tWordNetLemmatizer\n",
        "# # nltk.download('wordnet')\n",
        "\n",
        "# porter_stemmer  = PorterStemmer()\n",
        "# wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def get_wordnet_pos(word):\n",
        "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "#     tag_dict = {\"J\": wordnet.ADJ,\n",
        "#                 \"N\": wordnet.NOUN,\n",
        "#                 \"V\": wordnet.VERB,\n",
        "#                 \"R\": wordnet.ADV}\n",
        "\n",
        "#     return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "# # 1. Init Lemmatizer\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# # 2. Lemmatize Single Word with the appropriate POS tag\n",
        "# words = [] \n",
        "# for word in data.index:\n",
        "#   words.append(lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(porter_stemmer.stem(word)))\n",
        "\n",
        "# # replace the data index with proccesed list of words\n",
        "# data.index = words \n",
        "# print(\"the number of words in main data is \", len(data.index), \"the number of unique words after processing is \",  len(pd.DataFrame(words).drop_duplicates()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vte6tFx3GjMS"
      },
      "source": [
        "Does well but pos tags is a further step forward: unique words 7108"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS1uAs-DRz21"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "porter_stemmer  = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lList = []\n",
        "for w in data.index:\n",
        "  lList.append(wordnet_lemmatizer.lemmatize(porter_stemmer.stem(w)))\n",
        "\n",
        "data.index = lList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv4VBiHGAOnO"
      },
      "source": [
        "Spacy takes the longest to process: 7108 unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wemyfEqE7RSm"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "\n",
        "# # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# words3 = []\n",
        "# for w in data.index:\n",
        "#   doc = nlp(porter_stemmer.stem(w))\n",
        "#   words3.append([token.lemma_ for token in doc])\n",
        "\n",
        "\n",
        "# len(pd.DataFrame(words2).drop_duplicates())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME7XvMECA2oZ"
      },
      "source": [
        "# Preview of commonly used words accross products\n",
        "* gather new list of the most commonly used words across products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7prjVyyq0q0"
      },
      "outputs": [],
      "source": [
        "# Find the top 30 words said by each review\n",
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False).head(30)\n",
        "    top_dict[c]= list(zip(top.index, top.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QPw_qWzsS1H"
      },
      "outputs": [],
      "source": [
        "# Print the top 15 words said by each review\n",
        "for review, top_words in top_dict.items():\n",
        "    print(review)\n",
        "    print(', '.join([word for word, count in top_words[0:14]]))\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfLoyfHVnc93"
      },
      "source": [
        "**NOTE:** At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxJd-1v_siU9"
      },
      "outputs": [],
      "source": [
        "# Look at the most common top words --> add them to the stop word list\n",
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 30 words for each review\n",
        "words = []\n",
        "for review in data.columns:\n",
        "    top = [word for (word, count) in top_dict[review]]\n",
        "    for t in top:\n",
        "        words.append(t)   \n",
        "# aggregate the list and identify the most common words by how many times they occur\n",
        "Counter(words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKbkSUtiniyv"
      },
      "outputs": [],
      "source": [
        "# If more than X occurences, exclude it from the list\n",
        "add_stop_words = [word for word, count in Counter(words).most_common() if count > 15]\n",
        "add_stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyZwD-obVhAV"
      },
      "source": [
        "## Update dataset with combined new stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HexMHFhnv-O"
      },
      "outputs": [],
      "source": [
        "# Let's update our document-term matrix with the new list of stop words\n",
        "from sklearn.feature_extraction import text \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Read in cleaned data\n",
        "data_clean = pd.read_pickle('/content/drive/MyDrive/Data files/GamestopFile/clean_text_corpus.pkl').to_frame()\n",
        "# Add new stop words\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# Recreate document-term matrix\n",
        "cv = CountVectorizer(stop_words=stop_words)\n",
        "data_cv = cv.fit_transform(data_clean.review)\n",
        "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_stop.index = data_clean.index\n",
        "\n",
        "# Pickle it for later use\n",
        "import pickle\n",
        "pickle.dump(cv, open(\"/content/drive/MyDrive/Data files/GamestopFile/cv_stop.pkl\", \"wb\"))\n",
        "data_stop.to_pickle(\"/content/drive/MyDrive/Data files/GamestopFile/nameDTM.pkl\")\n",
        "\n",
        "data_stop.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGM5w0RlWtpU"
      },
      "source": [
        "# Add sentiment and polarity columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by-fIdzD1Yim"
      },
      "outputs": [],
      "source": [
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "data_clean['polarity'] = data_clean['review'].apply(pol)\n",
        "data_clean['subjectivity'] = data_clean['review'].apply(sub)\n",
        "data_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMooZESF54U"
      },
      "source": [
        "# Sentiment max and min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0GSoYx83Akk"
      },
      "outputs": [],
      "source": [
        "print('Most Positive product', data_clean.polarity.idxmax(), \"=\", data_clean.polarity.max())\n",
        "print('Most negative product', data_clean.polarity.idxmin(), \"=\", data_clean.polarity.min() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_bROvG017Zn"
      },
      "outputs": [],
      "source": [
        "observePolarity = data_clean.reset_index()\n",
        "observePolarity[[\"name\",\"polarity\"]].sort_values(\"polarity\", ascending = False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIbpW6YU1ZpB"
      },
      "outputs": [],
      "source": [
        "# Let's plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "for index, product in enumerate(data_clean.index):\n",
        "    x = data_clean.polarity.loc[product]\n",
        "    y = data_clean.subjectivity.loc[product]\n",
        "    plt.scatter(x, y, color='blue')\n",
        "    plt.text(x+.001, y+.001, index, fontsize=10)\n",
        "    # plt.xlim(-.01, .12) \n",
        "    \n",
        "plt.title('Sentiment Analysis', fontsize=20)\n",
        "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
        "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhL0dN3A62S6"
      },
      "source": [
        "# wordcloud preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHgFIDcyoqUv"
      },
      "outputs": [],
      "source": [
        "# word clouds!\n",
        "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0nxgAl63uLY"
      },
      "outputs": [],
      "source": [
        "n = 23\n",
        "wc.generate(data_clean.review[n])\n",
        "    \n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(data_clean.index[n])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiymEzJqrbei"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Data files/GamestopFile/sufficient_reviews.csv')\n",
        "data = data.sort_values('name', ascending = False)\n",
        "ndata_clean = data_clean.reset_index().sort_values(\"name\",ascending = False)\n",
        "data[\"review_polarity\"]=list(ndata_clean[\"polarity\"])\n",
        "data[\"review_subjectivity\"]=list(ndata_clean[\"subjectivity\"])\n",
        "data.to_csv(\"/content/drive/MyDrive/Data files/GamestopFile/final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2oNxbqAqjpl"
      },
      "source": [
        "Plot a bar chart comparing sentiment columns. Recomended sentiment was halfed for visual analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsyw9fHsu6HV"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFD5y57cEMkA"
      },
      "outputs": [],
      "source": [
        "data = data.sort_values('review_polarity', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUUgdSIUoiex"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "X = data.name\n",
        "Y = data.review_polarity\n",
        "Z = data.recommended_sentiment/2\n",
        "  \n",
        "X_axis = np.arange(len(X))\n",
        "  \n",
        "plt.bar(X_axis - 0.2, Y, 0.4, label = 'review_polarity')\n",
        "plt.bar(X_axis + 0.2, Z, 0.4, label = 'recommended_sentiment')\n",
        "  \n",
        "plt.xticks(X_axis, X, rotation= 90)\n",
        "plt.xlabel(\"Groups\")\n",
        "plt.ylabel(\"Sentiment\")\n",
        "plt.title(\"Product\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA0cWLrAHrUl"
      },
      "source": [
        "# Summary of product sentiment analisys\n",
        "\n",
        "> With the amount of words in some of the product reviews the results varied having higher range variances relative to the number of word counts.\n",
        "* data acquisisition was cleared for products less then 100 reviews \n",
        "* non game products functionality that meets the recomended requirements therefore stands out in the visual analysis\n",
        "* The focus of sentiment analysis is to find pos or neg features in games \n",
        "\n",
        "> Also some strong sentiment words were removed as stop words after lemmatizing and dumping into stopwordfile, stop words should be removed and saved before lematizing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txu7g90ypZ_b"
      },
      "source": [
        "# Topic Modeling\n",
        "\n",
        "> an attempt at analyzing a topic of one negative product review which in this case is the Samsung 49-in Super Ultra-Wide Dual QHD "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAaOHS9SdtVZ"
      },
      "outputs": [],
      "source": [
        "# Create a new document-term matrix using only nouns\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Let's create a function to pull out nouns from a string of text\n",
        "def nouns_adj(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
        "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "    tokenized = word_tokenize(text)\n",
        "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
        "    return ' '.join(nouns_adj)\n",
        "\n",
        "def nouns(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
        "    is_noun = lambda pos: pos[:2] == 'NN'\n",
        "    tokenized = word_tokenize(text)\n",
        "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
        "    return ' '.join(all_nouns)\n",
        "\n",
        "def setup(product_data, num_topics, passes):\n",
        "  add_stop_words = ['play', 'game', 'great', 'love', 'like', 'good', 'really', 'fun']\n",
        "  dataDTM = pd.DataFrame()\n",
        "  for j in range(3):\n",
        "    if (j ==0):\n",
        "      # drop stop words \n",
        "      cv = CountVectorizer(stop_words='english')\n",
        "      data = cv.fit_transform(product_data.review)\n",
        "      # Set new dataframe\n",
        "      dataDTM = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\n",
        "      dataDTM.index = product_data.index\n",
        "      # record stopwords\n",
        "      pickle.dump(cv, open(\"/content/drive/MyDrive/Data files/GamestopFile/stopwords.pkl\", \"wb\"))\n",
        "  \n",
        "    elif (j == 1):\n",
        "      # Apply the nouns function to the transcripts to filter only on nouns\n",
        "      product_data = pd.DataFrame(product_data.review.apply(nouns))\n",
        "      # Re-add the additional stop words since we are recreating the document-term matrix\n",
        "      stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "      cv= CountVectorizer(stop_words=stop_words)\n",
        "      data = cv.fit_transform(product_data.review)\n",
        "      # Set new dataframe\n",
        "      dataDTM = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\n",
        "      dataDTM.index = product_data.index\n",
        "      # record stopwords\n",
        "      pickle.dump(cv, open(\"/content/drive/MyDrive/Data files/GamestopFile/stopwords.pkl\", \"wb\"))\n",
        "  \n",
        "    elif (j==2):\n",
        "      product_data = pd.DataFrame(product_data.review.apply(nouns_adj))\n",
        "      stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "      cv = CountVectorizer(stop_words=stop_words)\n",
        "      data = cv.fit_transform(product_data.review)\n",
        "      # Set new dataframe\n",
        "      dataDTM = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\n",
        "      dataDTM.index = product_data.index\n",
        "      # record stopwords\n",
        "      pickle.dump(cv, open(\"/content/drive/MyDrive/Data files/GamestopFile/stopwords.pkl\", \"wb\"))\n",
        "  \n",
        "\n",
        "    # grab our clean data agin\n",
        "    dtm = dataDTM.transpose()\n",
        "    # We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
        "    sparse_counts = scipy.sparse.csr_matrix(dtm)\n",
        "    corpus = matutils.Sparse2Corpus(sparse_counts)\n",
        "    # Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
        "    cv = pickle.load(open(\"/content/drive/MyDrive/Data files/GamestopFile/stopwords.pkl\", \"rb\"))\n",
        "    id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
        "    lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, passes=passes)\n",
        "    if(j==0):\n",
        "       print(\"raw\")\n",
        "    if(j==1):\n",
        "       print(\"Nouns\")\n",
        "    if(j==2):\n",
        "       print(\"Nouns and adjectives\")\n",
        "    for i in lda.print_topics():\n",
        "      print(i)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyNX7PMXoU_T"
      },
      "source": [
        "I picked out 4 games to observe review topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICztn3CBVofO"
      },
      "outputs": [],
      "source": [
        "ndata= pd.read_pickle(\"/content/drive/MyDrive/Data files/GamestopFile/rawText_corpus.pkl\")\n",
        "MK = pd.DataFrame([[\"MortalKombat\",ndata.review.loc[\"Mortal Kombat Vs. DC Universe - PlayStation 3\"]]],columns = (\"name\",\"review\"))\n",
        "Zelda = pd.DataFrame([[\"Zelda\",ndata.review.loc[\"The Legend of Zelda: A Link Between Worlds - Nintendo 3DS\"]]],columns = (\"name\",\"review\"))\n",
        "Forza = pd.DataFrame([[\"Forza\",ndata.review.loc[\"Forza Horizon 4 - Xbox One\"]]],columns = (\"name\",\"review\"))\n",
        "Yoshi = pd.DataFrame([[\"Yoshi\",ndata.review.loc[\"Yoshi's Crafted World - Nintendo Switch\"]]],columns = (\"name\",\"review\")).set_index(\"name\")\n",
        "Thief = pd.DataFrame([[\"Thief\",ndata.review.loc[\"Thief - Xbox One\"]]],columns = (\"name\",\"review\")).set_index(\"name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sLwNJWFwmhB"
      },
      "source": [
        "raw topics looks best\n",
        "* topic 0: Fun and easy\n",
        "* topic 1: slightly challenging levels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvjHAzvpZVD"
      },
      "outputs": [],
      "source": [
        "#explore num topics\n",
        "setup(Yoshi, 2, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuX39QT1WbQ7"
      },
      "source": [
        "nouns\n",
        "* topic 0: Zelda fans\n",
        "* topic 1: Game design "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wrSrr63xSxa"
      },
      "outputs": [],
      "source": [
        "#explore num topics\n",
        "setup(Zelda, 2, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoOk4q25YSDT"
      },
      "source": [
        "I suppose just not enough reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD_rD-PUXLdg"
      },
      "outputs": [],
      "source": [
        "#explore num topics\n",
        "setup(Forza, 1, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPqk6pMkYOrv"
      },
      "source": [
        "2 topics doesnt look good\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cON1H8XJYBKf"
      },
      "outputs": [],
      "source": [
        "#explore num topics\n",
        "setup(MK, 2, 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5euMqMrHWNj"
      },
      "outputs": [],
      "source": [
        "#explore num topics\n",
        "setup(Thief, 2, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c0PfqgiREf4"
      },
      "source": [
        "# Text Generation: generate a review according to this product...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsdjOAfdhNjW"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle('/content/drive/MyDrive/Data files/GamestopFile/rawText_corpus.pkl')\n",
        "# ndata = data.sort_values(\"review_polarity\", ascending = False).set_index(\"name\")\n",
        "gMK = ndata.review.loc[\"Mortal Kombat Vs. DC Universe - PlayStation 3\"]\n",
        "gZelda = ndata.review.loc[\"The Legend of Zelda: A Link Between Worlds - Nintendo 3DS\"]\n",
        "gForza = ndata.review.loc[\"Forza Horizon 4 - Xbox One\"]\n",
        "gYoshi = ndata.review.loc[\"Yoshi's Crafted World - Nintendo Switch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILh9d-nNTcFm"
      },
      "source": [
        "## Build a Markov Chain Function\n",
        "\n",
        "We are going to build a simple Markov chain function that creates a dictionary:\n",
        "\n",
        "> The keys should be all of the words in the corpus\n",
        "\n",
        "> The values should be a list of the words that follow the keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-cmORK_SAT8"
      },
      "outputs": [],
      "source": [
        "def markov_chain(text):\n",
        "    '''The input is a string of text and the output will be a dictionary with each word as\n",
        "       a key and each value as the list of words that come after the key in the text.'''\n",
        "    \n",
        "    # Tokenize the text by word, though including punctuation\n",
        "    words = text.split(' ') \n",
        "    # Initialize a default dictionary to hold all of the words and next words\n",
        "    m_dict = defaultdict(list)\n",
        "    \n",
        "    # Create a zipped list of all of the word pairs and put them in word: list of next words format\n",
        "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
        "        m_dict[current_word].append(next_word)\n",
        "\n",
        "    # Convert the default dict back into a dictionary\n",
        "    m_dict = dict(m_dict)\n",
        "    return m_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8lAwV_2TlYS"
      },
      "outputs": [],
      "source": [
        "# Create the dictionary for Ali's routine, take a look at it\n",
        "YoshiDict = markov_chain(gYoshi)\n",
        "ZeldaDict = markov_chain(gZelda)\n",
        "ForzaDict = markov_chain(gForza)\n",
        "MKDict = markov_chain(gMK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg9gNNy2Txxv"
      },
      "source": [
        "## Create a Text Generator\n",
        "\n",
        "We're going to create a function that generates sentences. It will take two things as inputs:\n",
        "* The dictionary you just created\n",
        "* The number of words you want generated\n",
        "\n",
        "Here are some examples of generated sentences:\n",
        "\n",
        ">'Shape right turn– I also takes so that she’s got women all know that snail-trail.'\n",
        "\n",
        ">'Optimum level of early retirement, and be sure all the following Tuesday… because it’s too.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Jlk8aATmvv"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(chain, count=20):\n",
        "    '''Input a dictionary in the format of key = current word, value = list of next words\n",
        "       along with the number of words you would like to see in your generated sentence.'''\n",
        "\n",
        "    # Capitalize the first word\n",
        "    word1 = random.choice(list(chain.keys()))\n",
        "    sentence = word1.capitalize()\n",
        "\n",
        "    # Generate the second word from the value list. Set the new word as the first word. Repeat.\n",
        "    for i in range(count-1):\n",
        "        word2 = random.choice(chain[word1])\n",
        "        word1 = word2\n",
        "        sentence += ' ' + word2\n",
        "\n",
        "    # End it with a period\n",
        "    sentence += '.'\n",
        "    return(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GzKBkGuT39w"
      },
      "outputs": [],
      "source": [
        "generate_sentence(ForzaDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcp_35hZYnaD"
      },
      "source": [
        "All the puctuation is removed for the cost of cleaner markov pairing but a new cleaning stage may prove useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZNHTD8PT4Vz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP303Paku_Johnson_Assessment2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
